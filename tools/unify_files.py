#!/usr/bin/env python3
"""
Project Unification Tool (Enhanced)
====================================

This script consolidates all text-based files in a project into a single structured file.
It is designed to be an intelligent context engineering tool for AI assistants.

Features:
- Directory tree visualization for topographical awareness
- Configurable exclusions via .unifyignore files (gitignore syntax)
- Semantic truncation for large files (head/tail sampling)
- Structured XML output for unambiguous parsing
- Git metadata integration for temporal context
- Robust CLI with extensive configuration options
- Schema extraction for data files (JSON/CSV)
- Self-exclusion and binary file detection

Usage:
    python3 unify_files.py [source_directory] [options]

Examples:
    python3 unify_files.py
    python3 unify_files.py /path/to/project --output context.xml
    python3 unify_files.py --exclude-pattern "*.md" --verbose
    python3 unify_files.py --dry-run

If [source_directory] is not provided, the current working directory is used.
"""

import argparse
import csv
import datetime
import json
import os
import subprocess
import sys
from typing import Optional, Dict, List, Tuple

try:
    import pathspec
except ImportError:
    pathspec = None
    print("Warning: pathspec not installed. .unifyignore support disabled.", file=sys.stderr)
    print("Install with: pip install pathspec", file=sys.stderr)

# --- Configuration ---

# Max file size to process (2 MB)
MAX_FILE_SIZE = 2 * 1024 * 1024

# Truncation threshold for semantic sampling (1 MB)
TRUNCATE_THRESHOLD = 1 * 1024 * 1024

# Lines to sample from large files
HEAD_LINES = 50
TAIL_LINES = 100

# Max file size for log files (10 KB) - to prevent token overflow for AI analysis
MAX_LOG_FILE_SIZE = 10 * 1024

# Filename for external configuration
UNIFYIGNORE_FILENAME = ".unifyignore"

# Fallback exclusions if no .unifyignore is found
DEFAULT_EXCLUDED_DIRS = {
    ".git",
    ".svn",
    ".hg",
    ".idea",
    ".vscode",
    ".settings",
    "__pycache__",
    "venv",
    "env",
    ".venv",
    ".env",
    "node_modules",
    "dist",
    "build",
    "target",
    "bin",
    "obj",
    "site-packages",
    ".pytest_cache",
    ".mypy_cache",
    ".ruff_cache",
    "htmlcov",
    "cover",
    "coverage",
    "logs",
    "data_cache",  # Note: 'data/' contains source code, only exclude 'data_cache/'
}

# Patterns for data files that tend to be large or contain token-heavy content
DATA_FILE_PATTERNS = {
    ".log",
    ".csv",
    ".json",
    ".jsonl",
    ".tsv",
    ".parquet",
    ".arrow",
    ".feather",
    ".hdf5",
    ".h5",
    ".npy",
    ".npz",
    ".pkl",
    ".pickle",
    ".msgpack",
}

# Extensions that get stricter size limits
SIZE_RESTRICTED_EXTENSIONS = {
    ".log",
    ".csv",
    ".json",
    ".jsonl",
    ".tsv",
    ".txt",
}

DEFAULT_EXCLUDED_EXTENSIONS = {
    # Images
    ".jpg",
    ".jpeg",
    ".png",
    ".gif",
    ".bmp",
    ".tiff",
    ".ico",
    ".svg",
    ".webp",
    # Audio/Video
    ".mp3",
    ".wav",
    ".mp4",
    ".avi",
    ".mov",
    ".flv",
    ".mkv",
    # Documents/Binary
    ".pdf",
    ".doc",
    ".docx",
    ".xls",
    ".xlsx",
    ".ppt",
    ".pptx",
    ".zip",
    ".tar",
    ".gz",
    ".7z",
    ".rar",
    ".exe",
    ".dll",
    ".so",
    ".dylib",
    ".class",
    ".jar",
    ".pyc",
    ".pyo",
    ".pyd",
    ".db",
    ".sqlite",
    ".sqlite3",
    ".model",
    ".h5",
    ".onnx",
    ".pt",
    ".pth",
}

# Tag to identify files generated by this script to prevent recursion
GENERATED_FILE_TAG = "THIS_IS_A_UNIFIED_PROJECT_FILE_GENERATED_BY_TOOLS"


def is_binary(file_path: str) -> bool:
    """
    Check if a file is binary by reading a small chunk and checking for null bytes.
    Also handles empty files.
    """
    try:
        with open(file_path, "rb") as f:
            chunk = f.read(1024)
            if b"\0" in chunk:
                return True
            # Double check with robust encoding try
            try:
                chunk.decode("utf-8")
            except UnicodeDecodeError:
                return True
    except Exception:
        return True  # Treat inaccessible as binary/skip
    return False


def get_project_name(root_path: str) -> str:
    """
    Determine a suitable project name from the directory path.
    """
    return os.path.basename(os.path.abspath(root_path))


def extract_git_metadata(root_path: str) -> Dict[str, Optional[str]]:
    """
    Extract git metadata for temporal context.
    Returns dict with branch, sha, is_dirty, and error (if any).
    """
    metadata = {
        "branch": None,
        "sha": None,
        "is_dirty": False,
        "error": None,
    }

    try:
        # Check if we're in a git repository
        result = subprocess.run(
            ["git", "rev-parse", "--git-dir"],
            cwd=root_path,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode != 0:
            metadata["error"] = "Not a git repository"
            return metadata

        # Get current branch
        result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            cwd=root_path,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            metadata["branch"] = result.stdout.strip()

        # Get current commit SHA
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            cwd=root_path,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            metadata["sha"] = result.stdout.strip()[:12]  # Short SHA

        # Check for uncommitted changes
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            cwd=root_path,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            metadata["is_dirty"] = bool(result.stdout.strip())

    except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
        metadata["error"] = str(e)

    return metadata


def generate_directory_tree(
    root_path: str, 
    spec: Optional["pathspec.PathSpec"] = None,
    max_depth: int = 5,
    exclude_dirs: Optional[set] = None
) -> str:
    """
    Generate a visual directory tree respecting exclusion patterns.
    
    Args:
        root_path: Root directory to visualize
        spec: pathspec object for pattern matching
        max_depth: Maximum depth to traverse
        exclude_dirs: Set of directory names to exclude
        
    Returns:
        String representation of the directory tree
    """
    if exclude_dirs is None:
        exclude_dirs = DEFAULT_EXCLUDED_DIRS
        
    lines = []
    lines.append(f"ðŸ“ {os.path.basename(root_path)}/")
    
    def should_exclude(path: str, is_dir: bool = False) -> bool:
        """Check if path should be excluded."""
        rel_path = os.path.relpath(path, root_path)
        
        # Check pathspec
        if spec and spec.match_file(rel_path):
            return True
            
        # Check excluded directories
        if is_dir:
            basename = os.path.basename(path)
            if basename in exclude_dirs or basename.startswith("."):
                return True
                
        return False
    
    def walk_tree(current_path: str, prefix: str = "", depth: int = 0):
        """Recursively build tree structure."""
        if depth >= max_depth:
            return
            
        try:
            entries = []
            for entry in os.scandir(current_path):
                if should_exclude(entry.path, entry.is_dir()):
                    continue
                entries.append(entry)
            
            # Sort: directories first, then files, alphabetically
            entries.sort(key=lambda e: (not e.is_dir(), e.name.lower()))
            
            for i, entry in enumerate(entries):
                is_last = (i == len(entries) - 1)
                connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                extension = "    " if is_last else "â”‚   "
                
                icon = "ðŸ“" if entry.is_dir() else "ðŸ“„"
                lines.append(f"{prefix}{connector}{icon} {entry.name}")
                
                if entry.is_dir():
                    walk_tree(entry.path, prefix + extension, depth + 1)
                    
        except PermissionError:
            pass
    
    walk_tree(root_path)
    return "\n".join(lines)


def extract_schema_summary(file_path: str, ext: str) -> Optional[str]:
    """
    Extract schema/structure summary for data files.
    
    Args:
        file_path: Path to data file
        ext: File extension
        
    Returns:
        String summary of structure or None
    """
    try:
        if ext == ".json":
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, dict):
                    keys = list(data.keys())[:20]  # First 20 keys
                    return f"JSON Object with keys: {', '.join(keys)}"
                elif isinstance(data, list):
                    return f"JSON Array with {len(data)} elements"
                else:
                    return f"JSON {type(data).__name__}"
                    
        elif ext == ".csv":
            with open(file_path, "r", encoding="utf-8") as f:
                reader = csv.reader(f)
                headers = next(reader, None)
                if headers:
                    return f"CSV with columns: {', '.join(headers[:20])}"
                    
    except Exception:
        pass
        
    return None


def truncate_large_file(
    file_path: str,
    head_lines: int = HEAD_LINES,
    tail_lines: int = TAIL_LINES
) -> Tuple[str, bool]:
    """
    Truncate large files using head/tail sampling.
    
    Args:
        file_path: Path to file
        head_lines: Number of lines to keep from start
        tail_lines: Number of lines to keep from end
        
    Returns:
        Tuple of (content, was_truncated)
    """
    try:
        with open(file_path, "r", encoding="utf-8", errors="replace") as f:
            all_lines = f.readlines()
            
        total_lines = len(all_lines)
        
        if total_lines <= (head_lines + tail_lines):
            return "".join(all_lines), False
            
        # Sample head and tail
        head = all_lines[:head_lines]
        tail = all_lines[-tail_lines:]
        omitted = total_lines - head_lines - tail_lines
        
        marker = f"\n{'=' * 80}\n[TRUNCATED: {omitted} lines omitted]\n{'=' * 80}\n\n"
        
        content = "".join(head) + marker + "".join(tail)
        return content, True
        
    except Exception:
        return "", False


def load_ignore_patterns(
    root_path: str,
    config_path: Optional[str] = None
) -> Optional["pathspec.PathSpec"]:
    """
    Load ignore patterns from .unifyignore file.
    
    Args:
        root_path: Root directory to search for .unifyignore
        config_path: Optional explicit path to config file
        
    Returns:
        pathspec.PathSpec object or None
    """
    if pathspec is None:
        return None
        
    # Determine config file path
    if config_path:
        ignore_file = config_path
    else:
        ignore_file = os.path.join(root_path, UNIFYIGNORE_FILENAME)
        
    if not os.path.isfile(ignore_file):
        return None
        
    try:
        with open(ignore_file, "r", encoding="utf-8") as f:
            patterns = f.read()
        return pathspec.PathSpec.from_lines("gitwildmatch", patterns.splitlines())
    except Exception as e:
        print(f"Warning: Could not load {ignore_file}: {e}", file=sys.stderr)
        return None


def generate_header(
    root_path: str,
    output_filename: str,
    git_metadata: Dict[str, Optional[str]],
    tree: str,
    args: argparse.Namespace
) -> str:
    """
    Create the global explanatory header with metadata.
    """
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    lines = [
        "=" * 80,
        "PROJECT UNIFICATION FILE (Enhanced)",
        f"Generated: {now}",
        f"Source Directory: {os.path.abspath(root_path)}",
        f"Origin Script: {os.path.basename(__file__)}",
        f"Tag: {GENERATED_FILE_TAG}",
        "-" * 80,
    ]
    
    # Git Context Section
    lines.append("GIT CONTEXT:")
    if git_metadata["error"]:
        lines.append(f"  Status: {git_metadata['error']}")
    else:
        lines.append(f"  Branch: {git_metadata['branch']}")
        lines.append(f"  Commit: {git_metadata['sha']}")
        lines.append(f"  Dirty: {'Yes (uncommitted changes)' if git_metadata['is_dirty'] else 'No'}")
    
    lines.append("-" * 80)
    
    # Configuration
    lines.append("CONFIGURATION:")
    lines.append(f"  Max File Size: {MAX_FILE_SIZE / 1024 / 1024:.1f} MB")
    lines.append(f"  Truncation Threshold: {TRUNCATE_THRESHOLD / 1024 / 1024:.1f} MB")
    lines.append(f"  Head Lines: {args.head_lines if hasattr(args, 'head_lines') else HEAD_LINES}")
    lines.append(f"  Tail Lines: {args.tail_lines if hasattr(args, 'tail_lines') else TAIL_LINES}")
    lines.append(f"  Truncation: {'Enabled' if not args.no_truncate else 'Disabled'}")
    
    lines.append("-" * 80)
    
    # Instructions
    lines.extend([
        "INSTRUCTIONS:",
        "1. This file contains a structured representation of multiple project files.",
        "2. Each file is enclosed in <file> tags with metadata attributes.",
        "3. Large files are semantically truncated (head/tail sampling).",
        "4. Binary files and exclusion patterns are filtered out.",
        "5. The directory tree below provides navigational context.",
        "=" * 80,
        "",
        "DIRECTORY STRUCTURE:",
        tree,
        "=" * 80,
        "",
    ])
    
    return "\n".join(lines)


def get_file_type(file_path: str) -> str:
    """Determine file type from extension."""
    ext = os.path.splitext(file_path)[1].lower()
    
    type_map = {
        ".py": "python",
        ".js": "javascript",
        ".ts": "typescript",
        ".java": "java",
        ".cpp": "cpp",
        ".c": "c",
        ".h": "header",
        ".md": "markdown",
        ".txt": "text",
        ".json": "json",
        ".yaml": "yaml",
        ".yml": "yaml",
        ".toml": "toml",
        ".xml": "xml",
        ".html": "html",
        ".css": "css",
        ".sh": "shell",
        ".sql": "sql",
    }
    
    return type_map.get(ext, "unknown")


def scan_and_write(
    root_dir: str,
    output_file_path: str,
    args: argparse.Namespace
):
    """
    Main logic to scan directories and write to the output file.
    """
    start_time = datetime.datetime.now()
    project_name = get_project_name(root_dir)

    # Statistics
    stats = {
        "total_files_scanned": 0,
        "included_files": 0,
        "skipped_files": 0,
        "truncated_files": 0,
        "total_bytes_written": 0,
    }

    this_script_path = os.path.abspath(__file__)
    output_abspath = os.path.abspath(output_file_path)

    # Load ignore patterns
    spec = load_ignore_patterns(root_dir, args.config)
    
    # Extract git metadata
    git_metadata = extract_git_metadata(root_dir)
    
    # Generate directory tree
    tree = generate_directory_tree(root_dir, spec)

    if args.verbose:
        print(f"Starting unification of: {root_dir}")
        print(f"Output target: {output_file_path}")
        if spec:
            print(f"Loaded patterns from: {args.config or UNIFYIGNORE_FILENAME}")

    # Dry run mode
    if args.dry_run:
        print("\n=== DRY RUN MODE ===")
        print(f"Would generate tree:\n{tree}\n")
        print("Files that would be included:")

    try:
        # Open output file (or skip in dry-run)
        if not args.dry_run:
            outfile = open(output_file_path, "w", encoding="utf-8")
            # Write document start
            outfile.write("<project>\n")
            outfile.write(generate_header(root_dir, output_file_path, git_metadata, tree, args))
            outfile.write("\n")

        for current_root, dirs, files in os.walk(root_dir):
            # Prune excluded directories
            rel_dir = os.path.relpath(current_root, root_dir)
            
            # Filter directories
            filtered_dirs = []
            for d in dirs:
                dir_path = os.path.join(current_root, d)
                rel_path = os.path.relpath(dir_path, root_dir)
                
                # Check pathspec
                if spec and spec.match_file(rel_path):
                    continue
                    
                # Check default exclusions
                if d in DEFAULT_EXCLUDED_DIRS or d.startswith("."):
                    continue
                    
                # Apply additional exclude patterns from args
                if args.exclude_pattern and any(
                    pat in rel_path for pat in args.exclude_pattern
                ):
                    continue
                    
                filtered_dirs.append(d)
                
            dirs[:] = filtered_dirs

            for file in files:
                stats["total_files_scanned"] += 1
                file_path = os.path.join(current_root, file)
                abs_path = os.path.abspath(file_path)
                rel_path = os.path.relpath(file_path, root_dir)

                # --- Exclusion Logic ---

                # 1. Self-exclusion
                if abs_path == this_script_path or abs_path == output_abspath:
                    continue

                # 2. Pathspec exclusion
                if spec and spec.match_file(rel_path):
                    stats["skipped_files"] += 1
                    continue

                # 3. Extension exclusion
                _, ext = os.path.splitext(file)
                ext_lower = ext.lower()

                if ext_lower in DEFAULT_EXCLUDED_EXTENSIONS:
                    stats["skipped_files"] += 1
                    continue

                # 4. Data file pattern exclusion
                if ext_lower in DATA_FILE_PATTERNS:
                    stats["skipped_files"] += 1
                    continue

                # 5. Additional exclude patterns from args
                if args.exclude_pattern and any(
                    pat in rel_path for pat in args.exclude_pattern
                ):
                    stats["skipped_files"] += 1
                    continue

                # 6. Include patterns from args (if specified, only include matching)
                if args.include_pattern and not any(
                    pat in rel_path for pat in args.include_pattern
                ):
                    stats["skipped_files"] += 1
                    continue

                # 7. Size exclusion
                try:
                    file_size = os.path.getsize(file_path)

                    # Skip if too large (even for truncation)
                    if file_size > MAX_FILE_SIZE:
                        stats["skipped_files"] += 1
                        if args.verbose:
                            print(f"Skipping (too large): {rel_path}")
                        continue

                except OSError:
                    stats["skipped_files"] += 1
                    continue

                # 8. Binary check
                if is_binary(file_path):
                    stats["skipped_files"] += 1
                    continue

                # 9. Check for generated tag
                is_generated = False
                try:
                    with open(file_path, encoding="utf-8", errors="ignore") as check_f:
                        head = check_f.read(1024)
                        if GENERATED_FILE_TAG in head:
                            is_generated = True
                except Exception:
                    pass

                if is_generated:
                    stats["skipped_files"] += 1
                    continue

                # --- Process File ---
                stats["included_files"] += 1

                if args.verbose:
                    print(f"Processing: {rel_path}")
                elif not args.dry_run:
                    # Simple progress indicator
                    if stats["included_files"] % 10 == 0:
                        print(f"Processed {stats['included_files']} files...", end="\r")

                # Dry run: just list files
                if args.dry_run:
                    print(f"  {rel_path} ({file_size} bytes)")
                    continue

                # Read file content with optional truncation
                content = ""
                was_truncated = False
                
                # Check for schema extraction
                schema_summary = None
                if ext_lower in {".json", ".csv"}:
                    schema_summary = extract_schema_summary(file_path, ext_lower)

                # Apply truncation if enabled and file is large
                if not args.no_truncate and file_size > TRUNCATE_THRESHOLD:
                    content, was_truncated = truncate_large_file(
                        file_path,
                        args.head_lines if hasattr(args, 'head_lines') else HEAD_LINES,
                        args.tail_lines if hasattr(args, 'tail_lines') else TAIL_LINES
                    )
                    if was_truncated:
                        stats["truncated_files"] += 1
                else:
                    try:
                        with open(file_path, encoding="utf-8", errors="replace") as infile:
                            content = infile.read()
                    except Exception as e:
                        if args.verbose:
                            print(f"Error reading {file_path}: {e}")
                        stats["skipped_files"] += 1
                        stats["included_files"] -= 1
                        continue

                # Write structured output
                file_type = get_file_type(file_path)
                
                # XML-style file tag with attributes
                file_tag = f'<file path="{rel_path}" size="{file_size}" type="{file_type}"'
                if was_truncated:
                    file_tag += ' truncated="true"'
                if schema_summary:
                    # Escape quotes in schema summary
                    schema_escaped = schema_summary.replace('"', '&quot;')
                    file_tag += f' schema="{schema_escaped}"'
                file_tag += '>\n'
                
                outfile.write(file_tag)
                
                # Write schema summary as comment if available
                if schema_summary and not was_truncated:
                    outfile.write(f"<!-- Schema: {schema_summary} -->\n")
                
                outfile.write(content)
                
                # Ensure newline before closing tag
                if content and not content.endswith("\n"):
                    outfile.write("\n")
                    
                outfile.write("</file>\n\n")
                
                stats["total_bytes_written"] += len(file_tag) + len(content) + 10

        # Close document
        if not args.dry_run:
            outfile.write("</project>\n")
            outfile.close()

    except OSError as e:
        print(f"Critical Error writing output file: {e}")
        sys.exit(1)

    end_time = datetime.datetime.now()
    duration = end_time - start_time

    # --- Summary ---
    print("\n" + "=" * 40)
    if args.dry_run:
        print("DRY RUN COMPLETE")
    else:
        print("UNIFICATION COMPLETE")
    print("=" * 40)
    
    if not args.dry_run:
        print(f"Output File:    {output_file_path}")
    print(f"Scanning from:  {root_dir}")
    print(f"Time Taken:     {duration.total_seconds():.2f} seconds")
    print("-" * 40)
    print(f"Total Scanned:  {stats['total_files_scanned']}")
    print(f"Included:       {stats['included_files']}")
    print(f"Skipped:        {stats['skipped_files']}")
    if stats['truncated_files'] > 0:
        print(f"Truncated:      {stats['truncated_files']}")
    if not args.dry_run:
        print(f"Output Size:    {stats['total_bytes_written'] / 1024:.2f} KB")
    print("=" * 40)


def main():
    """Main entry point with argument parsing."""
    parser = argparse.ArgumentParser(
        description="Intelligent project context unification tool for AI assistants.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                          # Unify current directory
  %(prog)s /path/to/project         # Unify specific directory
  %(prog)s --output context.xml     # Custom output filename
  %(prog)s --exclude-pattern "*.md" # Exclude markdown files
  %(prog)s --dry-run --verbose      # See what would be included
  %(prog)s --no-truncate            # Disable semantic truncation
        """
    )
    
    parser.add_argument(
        "source_dir",
        nargs="?",
        default=os.getcwd(),
        help="Source directory to unify (default: current directory)"
    )
    
    parser.add_argument(
        "-o", "--output",
        help="Output file path (default: <project>_<timestamp>.txt)"
    )
    
    parser.add_argument(
        "-c", "--config",
        help="Path to custom .unifyignore file"
    )
    
    parser.add_argument(
        "--include-pattern",
        action="append",
        help="Include only files matching pattern (can be used multiple times)"
    )
    
    parser.add_argument(
        "--exclude-pattern",
        action="append",
        help="Exclude files matching pattern (can be used multiple times)"
    )
    
    parser.add_argument(
        "--no-truncate",
        action="store_true",
        help="Disable semantic truncation for large files"
    )
    
    parser.add_argument(
        "--head-lines",
        type=int,
        default=HEAD_LINES,
        help=f"Number of lines to keep from file start (default: {HEAD_LINES})"
    )
    
    parser.add_argument(
        "--tail-lines",
        type=int,
        default=TAIL_LINES,
        help=f"Number of lines to keep from file end (default: {TAIL_LINES})"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be included without writing output"
    )
    
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Verbose output with detailed progress"
    )
    
    args = parser.parse_args()
    
    # Validate source directory
    if not os.path.isdir(args.source_dir):
        print(f"Error: Directory '{args.source_dir}' not found.", file=sys.stderr)
        sys.exit(1)
    
    # Determine output filename
    if args.output:
        output_filename = args.output
    else:
        project_name = get_project_name(args.source_dir)
        timestamp = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
        output_filename = f"{project_name}_{timestamp}.txt"
    
    # Run the unification
    scan_and_write(args.source_dir, output_filename, args)


if __name__ == "__main__":
    main()
