{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "1886846a",
            "metadata": {},
            "source": [
                "# üåå x.titan: Autonomous Trading Brain - Kaggle Training\n",
                "\n",
                "**Version**: 2026.1.3 (Fixed)\n",
                "**Source**: GitHub ‚Üí https://github.com/planetazul3/x.titan\n",
                "\n",
                "## ‚öôÔ∏è Requirements\n",
                "1. **GPU Accelerator**: Enable in Kaggle Settings (T4 or P100)\n",
                "2. **Kaggle Secrets**: Add `DERIV_API_TOKEN` for data download\n",
                "3. **Internet**: Enable for GitHub clone + pip install"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e62abd14",
            "metadata": {},
            "source": [
                "## 1. Clone Repository from GitHub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eb785416",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "REPO_URL = \"https://github.com/planetazul3/x.titan.git\"\n",
                "WORKING_DIR = Path(\"/kaggle/working/x.titan\")\n",
                "\n",
                "if not WORKING_DIR.exists():\n",
                "    print(\"üì• Cloning x.titan from GitHub...\")\n",
                "    !git clone --depth 1 {REPO_URL} {WORKING_DIR}\n",
                "else:\n",
                "    print(\"‚úÖ Repository already exists\")\n",
                "\n",
                "os.chdir(WORKING_DIR)\n",
                "sys.path.insert(0, str(WORKING_DIR))\n",
                "print(f\"üìÇ Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0909b956",
            "metadata": {},
            "source": [
                "## 2. TA-Lib Installation (C Library)\n",
                "**Note**: Using single-threaded make to avoid race conditions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "09b7a6fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "set -e\n",
                "\n",
                "if [ ! -f /usr/include/ta-lib/ta_defs.h ]; then\n",
                "    echo \"üì¶ Installing TA-Lib...\"\n",
                "    apt-get update -qq && apt-get install -y -qq build-essential wget\n",
                "    wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
                "    tar -xzf ta-lib-0.4.0-src.tar.gz\n",
                "    cd ta-lib\n",
                "    ./configure --prefix=/usr > /dev/null\n",
                "    make > /dev/null  # Single-threaded to avoid race condition\n",
                "    make install > /dev/null\n",
                "    cd ..\n",
                "    rm -rf ta-lib ta-lib-0.4.0-src.tar.gz\n",
                "    echo \"‚úÖ TA-Lib compiled successfully\"\n",
                "else\n",
                "    echo \"‚úÖ TA-Lib already installed\"\n",
                "fi\n",
                "\n",
                "# Verify installation\n",
                "if [ -f /usr/lib/libta_lib.so ]; then\n",
                "    echo \"‚úÖ TA-Lib library found\"\n",
                "else\n",
                "    echo \"‚ùå TA-Lib installation failed!\"\n",
                "    exit 1\n",
                "fi"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eba53fe1",
            "metadata": {},
            "source": [
                "## 3. Python Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c748e3df",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üì• Installing Python packages...\")\n",
                "!pip install -q TA-Lib pandas numpy torch tqdm pydantic pydantic-settings python-dotenv pyarrow\n",
                "\n",
                "# Install forked deriv-api in editable mode\n",
                "import subprocess\n",
                "result = subprocess.run(\n",
                "    [\"pip\", \"install\", \"-q\", \"-e\", \"./python-deriv-api\"],\n",
                "    capture_output=True, text=True\n",
                ")\n",
                "if result.returncode != 0:\n",
                "    print(\"‚ö†Ô∏è Local install failed, trying from GitHub...\")\n",
                "    !pip install -q git+https://github.com/planetazul3/python-deriv-api.git\n",
                "\n",
                "# Verify critical imports\n",
                "try:\n",
                "    import talib\n",
                "    import deriv_api\n",
                "    print(\"‚úÖ Dependencies installed and verified\")\n",
                "except ImportError as e:\n",
                "    print(f\"‚ùå Import failed: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "da839111",
            "metadata": {},
            "source": [
                "## 4. Environment Configuration (API Key via Kaggle Secrets)\n",
                "\n",
                "**‚ö†Ô∏è IMPORTANT**: Add your Deriv API token in Kaggle:\n",
                "1. Click **Add-ons** ‚Üí **Secrets**\n",
                "2. Add key: `DERIV_API_TOKEN` with your token value"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6161f045",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load API token from Kaggle secrets\n",
                "DERIV_API_TOKEN = \"\"\n",
                "try:\n",
                "    from kaggle_secrets import UserSecretsClient\n",
                "    secrets = UserSecretsClient()\n",
                "    DERIV_API_TOKEN = secrets.get_secret(\"DERIV_API_TOKEN\")\n",
                "    if DERIV_API_TOKEN:\n",
                "        print(\"‚úÖ DERIV_API_TOKEN loaded from Kaggle secrets\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è DERIV_API_TOKEN is empty - data download will fail\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Could not load secret: {e}\")\n",
                "    print(\"   Add DERIV_API_TOKEN via Kaggle Add-ons ‚Üí Secrets\")\n",
                "\n",
                "# Create .env file with all configuration\n",
                "env_content = f'''# x.titan Configuration (Generated by Kaggle Notebook)\n",
                "ENVIRONMENT=test\n",
                "DERIV_API_TOKEN={DERIV_API_TOKEN}\n",
                "DERIV_APP_ID=1089\n",
                "\n",
                "# Trading\n",
                "TRADING__SYMBOL=R_100\n",
                "TRADING__STAKE_AMOUNT=10.0\n",
                "\n",
                "# Probability Thresholds\n",
                "THRESHOLDS__CONFIDENCE_THRESHOLD_HIGH=0.80\n",
                "THRESHOLDS__LEARNING_THRESHOLD_MAX=0.70\n",
                "THRESHOLDS__LEARNING_THRESHOLD_MIN=0.50\n",
                "\n",
                "# TFT Hyperparameters (GPU Optimized)\n",
                "HYPERPARAMS__USE_TFT=True\n",
                "HYPERPARAMS__LEARNING_RATE=0.0007\n",
                "HYPERPARAMS__BATCH_SIZE=128\n",
                "HYPERPARAMS__LSTM_HIDDEN_SIZE=256\n",
                "HYPERPARAMS__CNN_FILTERS=128\n",
                "HYPERPARAMS__LATENT_DIM=64\n",
                "HYPERPARAMS__DROPOUT_RATE=0.2\n",
                "HYPERPARAMS__EPOCHS=30\n",
                "\n",
                "# Data Shapes\n",
                "DATA_SHAPES__SEQUENCE_LENGTH_TICKS=1000\n",
                "DATA_SHAPES__SEQUENCE_LENGTH_CANDLES=200\n",
                "DATA_SHAPES__WARMUP_STEPS=50\n",
                "'''\n",
                "\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(env_content)\n",
                "print(\"‚úÖ .env file created\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8c9bfad4",
            "metadata": {},
            "source": [
                "## 5. GPU Detection & Optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86fb47fc",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"GPU DETECTION\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "BATCH_SIZE = 16  # Default for CPU\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
                "    print(f\"   VRAM: {gpu_mem:.1f} GB\")\n",
                "    \n",
                "    # Optimize for available VRAM\n",
                "    if gpu_mem >= 15:\n",
                "        BATCH_SIZE = 256\n",
                "    elif gpu_mem >= 8:\n",
                "        BATCH_SIZE = 128\n",
                "    else:\n",
                "        BATCH_SIZE = 64\n",
                "    \n",
                "    print(f\"   Batch size: {BATCH_SIZE}\")\n",
                "    !nvidia-smi\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è NO GPU DETECTED - Training will be very slow!\")\n",
                "\n",
                "os.environ['HYPERPARAMS__BATCH_SIZE'] = str(BATCH_SIZE)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "513601ba",
            "metadata": {},
            "source": [
                "## 6. Data Download (12 Months)\n",
                "**Note**: This step requires a valid `DERIV_API_TOKEN` in Kaggle secrets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "119ba182",
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = Path('./data_cache')\n",
                "\n",
                "# Check if data already exists\n",
                "tick_files = list(DATA_PATH.glob('**/ticks/*.parquet')) if DATA_PATH.exists() else []\n",
                "\n",
                "if len(tick_files) >= 6:\n",
                "    print(f\"‚úÖ Found {len(tick_files)} tick files - skipping download\")\n",
                "    DATA_READY = True\n",
                "else:\n",
                "    print(\"üì° Downloading 12 months of historical data...\")\n",
                "    print(\"   (This takes 10-15 minutes)\")\n",
                "    \n",
                "    result = !python scripts/download_data.py --months 12 --symbol R_100 --output data_cache 2>&1\n",
                "    for line in result:\n",
                "        print(line)\n",
                "    \n",
                "    # Verify download succeeded\n",
                "    tick_files = list(DATA_PATH.glob('**/ticks/*.parquet')) if DATA_PATH.exists() else []\n",
                "    if len(tick_files) >= 6:\n",
                "        print(f\"\\n‚úÖ Download complete: {len(tick_files)} tick files\")\n",
                "        DATA_READY = True\n",
                "    else:\n",
                "        print(\"\\n‚ùå DATA DOWNLOAD FAILED\")\n",
                "        print(\"   Check that DERIV_API_TOKEN is set correctly in Kaggle secrets\")\n",
                "        print(\"   Go to Add-ons ‚Üí Secrets ‚Üí Add 'DERIV_API_TOKEN'\")\n",
                "        DATA_READY = False"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "09fefdf3",
            "metadata": {},
            "source": [
                "## 7. Training Pipeline\n",
                "**Note**: This cell will only run if data download succeeded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de7646a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "if not DATA_READY:\n",
                "    print(\"‚ùå SKIPPING TRAINING - No data available\")\n",
                "    print(\"   Fix the data download step first\")\n",
                "else:\n",
                "    # Ensure checkpoints directory exists\n",
                "    Path('checkpoints').mkdir(exist_ok=True)\n",
                "    \n",
                "    print(\"üöÄ STARTING TRAINING\")\n",
                "    print(\"=\" * 50)\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Execute training\n",
                "    !python scripts/train.py \\\n",
                "        --data-path data_cache \\\n",
                "        --epochs 30 \\\n",
                "        --checkpoint-dir checkpoints\n",
                "    \n",
                "    elapsed = time.time() - start_time\n",
                "    print(f\"\\n‚è±Ô∏è Training completed in {elapsed/60:.1f} minutes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c85052ef",
            "metadata": {},
            "source": [
                "## 8. Verify & Export Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "299facf9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from datetime import datetime\n",
                "\n",
                "best_model = Path('checkpoints/best_model.pt')\n",
                "\n",
                "if not best_model.exists():\n",
                "    print(\"‚ùå No checkpoint found!\")\n",
                "    print(\"   Training may not have completed successfully\")\n",
                "else:\n",
                "    size_mb = best_model.stat().st_size / 1e6\n",
                "    print(f\"‚úÖ best_model.pt: {size_mb:.1f} MB\")\n",
                "    \n",
                "    # Validate checkpoint\n",
                "    !python tools/verify_checkpoint.py --checkpoint checkpoints/best_model.pt\n",
                "    \n",
                "    # Create downloadable bundle\n",
                "    ts = datetime.now().strftime('%Y%m%d_%H%M')\n",
                "    bundle = f'xtitan_model_{ts}'\n",
                "    shutil.make_archive(bundle, 'zip', root_dir='.', base_dir='checkpoints')\n",
                "    print(f\"\\nüì¶ Download: {bundle}.zip from Output panel\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}