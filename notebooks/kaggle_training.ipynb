{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåå x.titan: Autonomous Trading Brain - Kaggle Training\n",
                "\n",
                "**Version**: 2026.1.1 (Post-Audit Hardened)\n",
                "**Objective**: Train the `DerivOmniModel` (Temporal Fusion Transformer) on historical Deriv market data.\n",
                "\n",
                "This notebook handles:\n",
                "1.  **TA-Lib Setup**: Compilation of technical indicator libraries.\n",
                "2.  **Environment Setup**: Reproducible configuration via `.env`.\n",
                "3.  **Smart Data Loading**: Auto-detection of Kaggle Input datasets to skip lengthy downloads.\n",
                "4.  **Hardware-Accelerated Training**: GPU-optimized execution of the hardened `scripts/train.py` pipeline."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. System Requirements & TA-Lib\n",
                "The x.titan system requires the TA-Lib C-library for technical indicator extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# 1.1 TA-Lib Installation\n",
                "if not os.path.exists('/usr/include/ta-lib/ta_defs.h'):\n",
                "    print(\"üì¶ Installing TA-Lib Build Dependencies...\")\n",
                "    !apt-get update > /dev/null\n",
                "    !apt-get install -y build-essential wget > /dev/null\n",
                "    \n",
                "    print(\"üì• Downloading TA-Lib source...\")\n",
                "    !wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
                "    !tar -xzf ta-lib-0.4.0-src.tar.gz\n",
                "    \n",
                "    print(\"üõ†Ô∏è Compiling TA-Lib (Approx 2 mins)...\")\n",
                "    os.chdir('ta-lib')\n",
                "    !./configure --prefix=/usr > /dev/null\n",
                "    !make > /dev/null\n",
                "    !make install > /dev/null\n",
                "    os.chdir('..')\n",
                "    !rm -rf ta-lib ta-lib-0.4.0-src.tar.gz\n",
                "    print(\"‚úÖ TA-Lib installed successfully!\")\n",
                "else:\n",
                "    print(\"‚úÖ TA-Lib already present in system headers.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Project & Code Setup\n",
                "Initialize the workspace and install Python dependencies. We use the custom `python-deriv-api` fork."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "\n",
                "# We assume the x.titan codebase is uploaded as a Kaggle Dataset called 'xtitan-source'\n",
                "SOURCE_DIR = Path('/kaggle/input/xtitan-source') \n",
                "WORKING_DIR = Path('/kaggle/working/xtitan')\n",
                "\n",
                "if not WORKING_DIR.exists():\n",
                "    print(f\"üöÄ Deploying project to {WORKING_DIR}...\")\n",
                "    if SOURCE_DIR.exists():\n",
                "        shutil.copytree(SOURCE_DIR, WORKING_DIR, \n",
                "                        ignore=shutil.ignore_patterns('venv', '__pycache__', '.git', 'data_cache', '.agent'))\n",
                "    else:\n",
                "        # If running from a uploaded zip in current directory\n",
                "        print(\"‚ö†Ô∏è Source not found in /kaggle/input, assuming current directory contains code.\")\n",
                "        # No-op, just ensure we are in the right place\n",
                "        WORKING_DIR = Path('/kaggle/working')\n",
                "\n",
                "os.chdir(WORKING_DIR)\n",
                "print(f\"üìÇ Current Directory: {os.getcwd()}\")\n",
                "\n",
                "print(\"üì• Installing Python Packages...\")\n",
                "!pip install -q TA-Lib pandas numpy torch matplotlib tqdm pydantic pydantic-settings python-dotenv\n",
                "!pip install -q ./python-deriv-api"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration (TFT Optimized)\n",
                "Configure the Temporal Fusion Transformer (TFT) hyperparameters and data shapes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile .env\n",
                "ENVIRONMENT=test\n",
                "TRADING__SYMBOL=R_100\n",
                "TRADING__STAKE_AMOUNT=10.0\n",
                "\n",
                "# Probability Thresholds (Post-Audit Standards)\n",
                "THRESHOLDS__CONFIDENCE_THRESHOLD_HIGH=0.80\n",
                "THRESHOLDS__LEARNING_THRESHOLD_MAX=0.70\n",
                "THRESHOLDS__LEARNING_THRESHOLD_MIN=0.50\n",
                "\n",
                "# TFT Hyperparameters\n",
                "HYPERPARAMS__USE_TFT=True\n",
                "HYPERPARAMS__LEARNING_RATE=0.0007\n",
                "HYPERPARAMS__BATCH_SIZE=256\n",
                "HYPERPARAMS__LSTM_HIDDEN_SIZE=256\n",
                "HYPERPARAMS__CNN_FILTERS=128\n",
                "HYPERPARAMS__LATENT_DIM=64\n",
                "HYPERPARAMS__DROPOUT_RATE=0.2\n",
                "HYPERPARAMS__EWC_SAMPLE_SIZE=5000\n",
                "\n",
                "# Data Shapes (Optimized for context)\n",
                "DATA_SHAPES__SEQUENCE_LENGTH_TICKS=1000\n",
                "DATA_SHAPES__SEQUENCE_LENGTH_CANDLES=200\n",
                "DATA_SHAPES__WARMUP_STEPS=50"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hardware Optimization & Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "print(\"--- Device Validation ---\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "    !nvidia-smi\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è GPU NOT DETECTED. Training will be extremely slow.\")\n",
                "    # Fallback batch size reduction for CPU OOM avoidance\n",
                "    os.environ['HYPERPARAMS__BATCH_SIZE'] = '16'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Sourcing (Smart Detection)\n",
                "We check for pre-bundled data in `/kaggle/input`. If missing, we download 12 months from the Deriv API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_INPUT_PATH = Path('/kaggle/input/xtitan-data/data_cache')\n",
                "DATA_LOCAL_PATH = Path('./data_cache')\n",
                "\n",
                "if DATA_INPUT_PATH.exists():\n",
                "    print(\"üîó Found bundled data in Kaggle Input. Creating symlink...\")\n",
                "    if DATA_LOCAL_PATH.exists():\n",
                "        if DATA_LOCAL_PATH.is_symlink(): os.unlink(DATA_LOCAL_PATH)\n",
                "        else: shutil.rmtree(DATA_LOCAL_PATH)\n",
                "    \n",
                "    # Symlink data to working directory for fast access\n",
                "    os.symlink(DATA_INPUT_PATH, DATA_LOCAL_PATH)\n",
                "    print(\"‚úÖ Data linked successfully.\")\n",
                "else:\n",
                "    print(\"üì° Data not found in input. Downloading 12 months (Scraping mode)...\")\n",
                "    # Note: Requires DERIV_API_TOKEN in secrets if downloading\n",
                "    !python scripts/download_data.py --months 12 --symbol R_100 --output data_cache"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Pre-Flight Validation\n",
                "Running the audited integrity scripts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python scripts/final_integrity_check.py\n",
                "!python pre_training_validation.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Training (Run Pipeline)\n",
                "Execute the hardened training loop. Results are saved to `./checkpoints`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clear previous stale logs/checkpoints\n",
                "!rm -rf checkpoints/*.pt\n",
                "!rm -rf logs/tensorboard/*\n",
                "\n",
                "# Execute audited training script\n",
                "!python scripts/train.py --data-path data_cache --epochs 50 --checkpoint-dir checkpoints"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export Artifacts\n",
                "Bundle model and Fisher Information for deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "\n",
                "ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
                "bundle_name = f\"xtitan_model_{ts}\"\n",
                "\n",
                "print(\"üì¶ Bundling artifacts...\")\n",
                "shutil.make_archive(bundle_name, 'zip', root_dir='.', base_dir='checkpoints')\n",
                "shutil.copy(\".env\", \"last_config.env\")\n",
                "\n",
                "print(f\"‚úÖ Done! Download {bundle_name}.zip from the output side-bar.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}