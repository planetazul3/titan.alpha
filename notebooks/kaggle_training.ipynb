{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "1886846a",
            "metadata": {},
            "source": [
                "# üåå x.titan: Autonomous Trading Brain - Kaggle Training\n",
                "\n",
                "**Version**: 2026.1.10\n",
                "**Source**: GitHub ‚Üí https://github.com/planetazul3/x.titan"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e62abd14",
            "metadata": {},
            "source": [
                "## 1. Clone Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eb785416",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, sys\n",
                "from pathlib import Path\n",
                "\n",
                "WORKING_DIR = Path('/kaggle/working/x.titan')\n",
                "if not WORKING_DIR.exists():\n",
                "    print('üì• Cloning x.titan...')\n",
                "    !git clone --depth 1 https://github.com/planetazul3/x.titan.git {WORKING_DIR}\n",
                "else:\n",
                "    print('‚úÖ Repo exists')\n",
                "\n",
                "os.chdir(WORKING_DIR)\n",
                "sys.path.insert(0, str(WORKING_DIR))\n",
                "print(f'üìÇ {os.getcwd()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0909b956",
            "metadata": {},
            "source": [
                "## 2. TA-Lib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "09b7a6fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "set -e\n",
                "if [ ! -f /usr/include/ta-lib/ta_defs.h ]; then\n",
                "    echo 'üì¶ Installing TA-Lib...'\n",
                "    apt-get update -qq && apt-get install -y -qq build-essential wget\n",
                "    wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
                "    tar -xzf ta-lib-0.4.0-src.tar.gz && cd ta-lib\n",
                "    ./configure --prefix=/usr > /dev/null && make > /dev/null && make install > /dev/null\n",
                "    cd .. && rm -rf ta-lib ta-lib-0.4.0-src.tar.gz\n",
                "    echo '‚úÖ TA-Lib compiled'\n",
                "else\n",
                "    echo '‚úÖ TA-Lib already installed'\n",
                "fi"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eba53fe1",
            "metadata": {},
            "source": [
                "## 3. Python Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c748e3df",
            "metadata": {},
            "outputs": [],
            "source": [
                "print('üì• Installing packages...')\n",
                "!pip install -q TA-Lib pandas numpy torch tqdm pydantic pydantic-settings python-dotenv pyarrow pandera\n",
                "\n",
                "import subprocess\n",
                "result = subprocess.run(['pip', 'install', '-q', '-e', './python-deriv-api'], capture_output=True)\n",
                "if result.returncode != 0:\n",
                "    !pip install -q git+https://github.com/planetazul3/python-deriv-api.git\n",
                "\n",
                "import talib, deriv_api, pandera\n",
                "print('‚úÖ Dependencies installed')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "da839111",
            "metadata": {},
            "source": [
                "## 4. Environment Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6161f045",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load API token (optional)\n",
                "DERIV_API_TOKEN = ''\n",
                "try:\n",
                "    from kaggle_secrets import UserSecretsClient\n",
                "    DERIV_API_TOKEN = UserSecretsClient().get_secret('DERIV_API_TOKEN') or ''\n",
                "    if DERIV_API_TOKEN: print('‚úÖ DERIV_API_TOKEN loaded')\n",
                "except: pass\n",
                "\n",
                "# FULL .env with ALL required fields\n",
                "env_content = f'''ENVIRONMENT=test\n",
                "DERIV_API_TOKEN={DERIV_API_TOKEN}\n",
                "DERIV_APP_ID=1089\n",
                "\n",
                "# Trading (REQUIRED)\n",
                "TRADING__SYMBOL=R_100\n",
                "TRADING__STAKE_AMOUNT=10.0\n",
                "TRADING__TIMEFRAME=1m\n",
                "\n",
                "# Safety\n",
                "EXECUTION_SAFETY__KILL_SWITCH_ENABLED=false\n",
                "EXECUTION_SAFETY__MAX_DAILY_LOSS=100.0\n",
                "EXECUTION_SAFETY__MAX_STAKE_PER_TRADE=10.0\n",
                "\n",
                "# Thresholds (REQUIRED)\n",
                "THRESHOLDS__CONFIDENCE_THRESHOLD_HIGH=0.80\n",
                "THRESHOLDS__LEARNING_THRESHOLD_MIN=0.50\n",
                "THRESHOLDS__LEARNING_THRESHOLD_MAX=0.70\n",
                "THRESHOLDS__CAUTION_THRESHOLD_MARGIN=0.05\n",
                "\n",
                "# Hyperparams\n",
                "HYPERPARAMS__USE_TFT=True\n",
                "HYPERPARAMS__LEARNING_RATE=0.0007\n",
                "HYPERPARAMS__BATCH_SIZE=128\n",
                "HYPERPARAMS__LSTM_HIDDEN_SIZE=256\n",
                "HYPERPARAMS__CNN_FILTERS=128\n",
                "HYPERPARAMS__LATENT_DIM=64\n",
                "HYPERPARAMS__DROPOUT_RATE=0.2\n",
                "HYPERPARAMS__EWC_SAMPLE_SIZE=2000\n",
                "\n",
                "# Data shapes\n",
                "DATA_SHAPES__SEQUENCE_LENGTH_TICKS=1000\n",
                "DATA_SHAPES__SEQUENCE_LENGTH_CANDLES=200\n",
                "DATA_SHAPES__WARMUP_STEPS=50\n",
                "'''\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(env_content)\n",
                "print('‚úÖ .env created')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8c9bfad4",
            "metadata": {},
            "source": [
                "## 5. GPU Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86fb47fc",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "# Conservative batch sizes to avoid OOM on T4 (16GB)\n",
                "BATCH_SIZE = 16\n",
                "if torch.cuda.is_available():\n",
                "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f'‚úÖ GPU: {torch.cuda.get_device_name(0)} ({gpu_mem:.1f} GB)')\n",
                "    # OOM fix: T4 with 15GB -> batch 128 max (not 256)\n",
                "    BATCH_SIZE = 128 if gpu_mem >= 15 else 64 if gpu_mem >= 8 else 32\n",
                "    print(f'   Batch size: {BATCH_SIZE}')\n",
                "else:\n",
                "    print('‚ö†Ô∏è NO GPU')\n",
                "\n",
                "os.environ['HYPERPARAMS__BATCH_SIZE'] = str(BATCH_SIZE)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "513601ba",
            "metadata": {},
            "source": [
                "## 6. Data Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "119ba182",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download from GitHub Release\n",
                "DATA_PATH = Path('./data_cache')\n",
                "\n",
                "if not DATA_PATH.exists():\n",
                "    print('üì• Downloading training data from GitHub Release...')\n",
                "    !wget -q https://github.com/planetazul3/x.titan/releases/download/v2026.1.8-data/xtitan_training_data_2025.zip\n",
                "    !unzip -q xtitan_training_data_2025.zip\n",
                "    !rm xtitan_training_data_2025.zip\n",
                "\n",
                "tick_files = list(DATA_PATH.glob('**/ticks/*.parquet'))\n",
                "candle_files = list(DATA_PATH.glob('**/candles*/*.parquet'))\n",
                "\n",
                "if len(tick_files) >= 6 and len(candle_files) >= 6:\n",
                "    print(f'‚úÖ Data ready: {len(tick_files)} tick files, {len(candle_files)} candle files')\n",
                "    DATA_READY = True\n",
                "else:\n",
                "    print('‚ùå Data extraction failed')\n",
                "    DATA_READY = False"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "09fefdf3",
            "metadata": {},
            "source": [
                "## 7. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de7646a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "if not DATA_READY:\n",
                "    print('‚ùå SKIPPING - No data')\n",
                "else:\n",
                "    Path('checkpoints').mkdir(exist_ok=True)\n",
                "    print('üöÄ STARTING TRAINING')\n",
                "    start = time.time()\n",
                "    \n",
                "    !python scripts/train.py \\\n",
                "        --data-path data_cache \\\n",
                "        --epochs 30 \\\n",
                "        --batch-size {BATCH_SIZE} \\\n",
                "        --checkpoint-dir checkpoints \\\n",
                "        --months 12\n",
                "    \n",
                "    print(f'\\n‚è±Ô∏è Completed in {(time.time()-start)/60:.1f} min')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c85052ef",
            "metadata": {},
            "source": [
                "## 8. Export Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "299facf9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from datetime import datetime\n",
                "\n",
                "best_model = Path('checkpoints/best_model.pt')\n",
                "\n",
                "if best_model.exists():\n",
                "    print(f'‚úÖ best_model.pt: {best_model.stat().st_size/1e6:.1f} MB')\n",
                "    # Fixed: verify_checkpoint.py uses positional arg, not --checkpoint\n",
                "    !python tools/verify_checkpoint.py checkpoints/best_model.pt\n",
                "    \n",
                "    ts = datetime.now().strftime('%Y%m%d_%H%M')\n",
                "    shutil.make_archive(f'xtitan_model_{ts}', 'zip', '.', 'checkpoints')\n",
                "    print(f'\\nüì¶ Download: xtitan_model_{ts}.zip')\n",
                "else:\n",
                "    print('‚ùå No checkpoint found')\n",
                "    print(f'   Existing: {list(Path(\"checkpoints\").glob(\"*.pt\"))}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "quick_test",
            "metadata": {},
            "source": [
                "## 9. Quick Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test_inference",
            "metadata": {},
            "outputs": [],
            "source": [
                "if Path('checkpoints/best_model.pt').exists():\n",
                "    import torch\n",
                "    from config.settings import load_settings\n",
                "    from models.core import DerivOmniModel\n",
                "    \n",
                "    settings = load_settings()\n",
                "    model = DerivOmniModel(settings)\n",
                "    # PyTorch 2.6+ requires weights_only=False for checkpoints with custom classes\n",
                "    ckpt = torch.load('checkpoints/best_model.pt', map_location='cpu', weights_only=False)\n",
                "    model.load_state_dict(ckpt['model_state_dict'])\n",
                "    model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        probs = model.predict_probs(\n",
                "            torch.randn(1, settings.data_shapes.sequence_length_ticks),\n",
                "            torch.randn(1, settings.data_shapes.sequence_length_candles, settings.data_shapes.feature_dim_candles),\n",
                "            torch.randn(1, settings.data_shapes.feature_dim_volatility)\n",
                "        )\n",
                "    print(f'‚úÖ Inference OK: rise_fall={probs[\"rise_fall_prob\"].item():.4f}')\n",
                "else:\n",
                "    print('‚è≠Ô∏è No model')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}