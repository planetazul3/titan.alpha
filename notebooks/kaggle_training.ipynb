{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous Trading Brain - Kaggle Training Pipeline\n",
    "\n",
    "This notebook sets up the environment, installs dependencies (including TA-Lib), prepares the data, and runs the training loop on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & TA-Lib Installation\n",
    "Compiling TA-Lib C-library (required for technical indicators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Install system build tools\n",
    "!apt-get update > /dev/null\n",
    "!apt-get install -y build-essential > /dev/null\n",
    "\n",
    "# Download and compile TA-Lib\n",
    "if not os.path.exists('/usr/lib/libta_lib.so'):\n",
    "    print(\"Downloading TA-Lib source...\")\n",
    "    !wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
    "    !tar -xzf ta-lib-0.4.0-src.tar.gz\n",
    "    \n",
    "    print(\"Compiling TA-Lib (this takes ~2 mins)...\")\n",
    "    os.chdir('ta-lib')\n",
    "    !./configure --prefix=/usr > /dev/null\n",
    "    !make > /dev/null\n",
    "    !make install > /dev/null\n",
    "    os.chdir('..')\n",
    "    print(\"TA-Lib installed successfully!\")\n",
    "else:\n",
    "    print(\"TA-Lib already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project Setup\n",
    "Copying code from Input to Working directory to allow execution and modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIG: Change this to match your uploaded dataset name\n",
    "INPUT_DIR = Path('/kaggle/input/xtitan-project/xtitan') \n",
    "WORKING_DIR = Path('/kaggle/working/xtitan')\n",
    "\n",
    "if not WORKING_DIR.exists():\n",
    "    print(f\"Copying project from {INPUT_DIR} to {WORKING_DIR}...\")\n",
    "    # Copy ignoring git/venv/cache if they exist\n",
    "    shutil.copytree(INPUT_DIR, WORKING_DIR, \n",
    "                    ignore=shutil.ignore_patterns('venv', '__pycache__', '.git', '*.optimize', 'data_cache'))\n",
    "    print(\"Project setup complete.\")\n",
    "else:\n",
    "    print(\"Project already in working directory.\")\n",
    "\n",
    "os.chdir(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install TA-Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "**Check**: Do we need to download data or did you upload it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Link uploaded data (if you uploaded data_cache as a dataset)\n",
    "UPLOADED_DATA = Path('/kaggle/input/xtitan-data/data_cache')\n",
    "LOCAL_DATA = Path('data_cache')\n",
    "\n",
    "if UPLOADED_DATA.exists():\n",
    "    print(f\"Found uploaded data at {UPLOADED_DATA}. Linking...\")\n",
    "    if LOCAL_DATA.exists():\n",
    "        shutil.rmtree(LOCAL_DATA)\n",
    "    shutil.copytree(UPLOADED_DATA, LOCAL_DATA)\n",
    "    print(\"Data linked successfully.\")\n",
    "\n",
    "else:\n",
    "    print(\"No uploaded data found. Downloading from Deriv...\")\n",
    "    # Ensure .env exists or set vars (Deriv API doesn't need key for public historical data usually, \n",
    "    # but if your script needs it, you should set secrets in Kaggle)\n",
    "    \n",
    "    # Example download command (adjust symbol/months)\n",
    "    !python scripts/download_data.py --months 12 --symbol R_100 --output data_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. System Validation\n",
    "Running pre-flight checks before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pre_training_validation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "Running the main training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous logs if needed\n",
    "# !rm -rf logs/\n",
    "\n",
    "# Run training\n",
    "!python scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Artifact Archival\n",
    "Zip results for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "archive_name = f\"training_artifacts_{timestamp}\"\n",
    "\n",
    "print(\"Archiving checkpoints and logs...\")\n",
    "shutil.make_archive(archive_name, 'zip', root_dir='.', base_dir='checkpoints')\n",
    "shutil.make_archive(f\"{archive_name}_logs\", 'zip', root_dir='.', base_dir='logs')\n",
    "\n",
    "print(f\"Created {archive_name}.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
