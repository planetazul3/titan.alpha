{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Autonomous Trading Brain - Kaggle Training Pipeline\n",
                "\n",
                "This notebook sets up the environment, installs dependencies (including TA-Lib and custom Deriv API), prepares the data (full scratch download), and runs the training loop on GPU."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup & TA-Lib Installation\n",
                "Compiling TA-Lib C-library (required for technical indicators)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Install system build tools\n",
                "!apt-get update > /dev/null\n",
                "!apt-get install -y build-essential > /dev/null\n",
                "\n",
                "# Download and compile TA-Lib\n",
                "if not os.path.exists('/usr/lib/libta_lib.so'):\n",
                "    print(\"Downloading TA-Lib source...\")\n",
                "    !wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
                "    !tar -xzf ta-lib-0.4.0-src.tar.gz\n",
                "    \n",
                "    print(\"Compiling TA-Lib (this takes ~2 mins)...\")\n",
                "    os.chdir('ta-lib')\n",
                "    !./configure --prefix=/usr > /dev/null\n",
                "    !make > /dev/null\n",
                "    !make install > /dev/null\n",
                "    os.chdir('..')\n",
                "    print(\"TA-Lib installed successfully!\")\n",
                "else:\n",
                "    print(\"TA-Lib already installed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Project Setup\n",
                "Copying code from Input to Working directory to allow execution and modification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from pathlib import Path\n",
                "\n",
                "# CONFIG: Change this to match your uploaded dataset name\n",
                "INPUT_DIR = Path('/kaggle/input/xtitan-project/xtitan') \n",
                "WORKING_DIR = Path('/kaggle/working/xtitan')\n",
                "\n",
                "if not WORKING_DIR.exists():\n",
                "    print(f\"Copying project from {INPUT_DIR} to {WORKING_DIR}...\")\n",
                "    # Copy ignoring git/venv/cache if they exist\n",
                "    shutil.copytree(INPUT_DIR, WORKING_DIR, \n",
                "                    ignore=shutil.ignore_patterns('venv', '__pycache__', '.git', '*.optimize', 'data_cache'))\n",
                "    print(\"Project setup complete.\")\n",
                "else:\n",
                "    print(\"Project already in working directory.\")\n",
                "\n",
                "os.chdir(WORKING_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Install Python Dependencies & Configure Environment\n",
                "Running `pip install` and generating a `.env` file for configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install TA-Lib\n",
                "!pip install -r requirements.txt\n",
                "\n",
                "# CORE: Install local custom Deriv API\n",
                "print(\"Installing custom python-deriv-api...\")\n",
                "!pip install ./python-deriv-api\n",
                "\n",
                "# CONFIGURATION STRATEGY:\n",
                "# 1. Check for uploaded .env in datasets (e.g. /kaggle/input/xtitan-env/.env)\n",
                "# 2. Check Kaggle Secrets for token\n",
                "# 3. Fallback to generating default .env\n",
                "\n",
                "from kaggle_secrets import UserSecretsClient\n",
                "import glob\n",
                "\n",
                "env_path = Path(\".env\")\n",
                "\n",
                "# Check for external .env files in input directories\n",
                "potential_env_files = glob.glob(\"/kaggle/input/**/.env\", recursive=True)\n",
                "\n",
                "if potential_env_files:\n",
                "    print(f\"Found external configuration: {potential_env_files[0]}\")\n",
                "    shutil.copy(potential_env_files[0], \".env\")\n",
                "else:\n",
                "    print(\"No external .env found. Generating default configuration...\")\n",
                "    \n",
                "    # Try to load API token from Kaggle Secrets\n",
                "    try:\n",
                "        user_secrets = UserSecretsClient()\n",
                "        api_token = user_secrets.get_secret(\"DERIV_API_TOKEN\")\n",
                "        print(\"Loaded DERIV_API_TOKEN from Kaggle Secrets.\")\n",
                "    except Exception:\n",
                "        print(\"No Kaggle Secret 'DERIV_API_TOKEN' found. Using empty string (config will fail if token needed for private methods).\")\n",
                "        api_token = \"\"\n",
                "\n",
                "    env_content = f\"\"\"\n",
                "TRADING__SYMBOL=R_100\n",
                "TRADING__STAKE_AMOUNT=10.0\n",
                "DERIV_API_TOKEN={api_token}\n",
                "\n",
                "THRESHOLDS__CONFIDENCE_THRESHOLD_HIGH=0.65\n",
                "THRESHOLDS__LEARNING_THRESHOLD_MIN=0.45\n",
                "THRESHOLDS__LEARNING_THRESHOLD_MAX=0.60\n",
                "\n",
                "HYPERPARAMS__LEARNING_RATE=0.0005\n",
                "HYPERPARAMS__BATCH_SIZE=64\n",
                "HYPERPARAMS__LSTM_HIDDEN_SIZE=128\n",
                "HYPERPARAMS__CNN_FILTERS=64\n",
                "HYPERPARAMS__LATENT_DIM=32\n",
                "\n",
                "DATA_SHAPES__SEQUENCE_LENGTH_TICKS=60\n",
                "DATA_SHAPES__SEQUENCE_LENGTH_CANDLES=60\n",
                "\"\"\"\n",
                "\n",
                "    with open(\".env\", \"w\") as f:\n",
                "        f.write(env_content)\n",
                "    \n",
                "    print(\"Created .env configuration file.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preparation (Fresh Download)\n",
                "Downloading 12 months of historical data from scratch. Previous data is cleared to ensure integrity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from pathlib import Path\n",
                "\n",
                "DATA_DIR = Path('data_cache')\n",
                "\n",
                "# CLEAN START: Remove existing data cache if any\n",
                "if DATA_DIR.exists():\n",
                "    print(\"Cleaning existing data cache...\")\n",
                "    shutil.rmtree(DATA_DIR)\n",
                "\n",
                "print(\"Downloading 12 months of data (This may take a while)...\")\n",
                "# Force download with --no-resume\n",
                "!python scripts/download_data.py --months 12 --symbol R_100 --output data_cache --no-resume"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. System Validation\n",
                "Running data integrity checks and pre-flight validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Final Integrity Check ---\")\n",
                "!python scripts/final_integrity_check.py\n",
                "\n",
                "print(\"\\n--- Pre-Training Validation ---\")\n",
                "!python pre_training_validation.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training\n",
                "Running the main training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clear previous logs if needed\n",
                "# !rm -rf logs/\n",
                "\n",
                "# Run training\n",
                "!python scripts/train.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Artifact Archival\n",
                "Zip results for download."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from datetime import datetime\n",
                "\n",
                "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "archive_name = f\"training_artifacts_{timestamp}\"\n",
                "\n",
                "print(\"Archiving checkpoints and logs...\")\n",
                "shutil.make_archive(archive_name, 'zip', root_dir='.', base_dir='checkpoints')\n",
                "shutil.make_archive(f\"{archive_name}_logs\", 'zip', root_dir='.', base_dir='logs')\n",
                "\n",
                "print(f\"Created {archive_name}.zip\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}