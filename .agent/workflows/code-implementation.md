---
description: Report Review Implementation Instructions
---

# DEVELOPMENT AI - CODE IMPLEMENTATION INSTRUCTIONS

## ROLE
Senior development AI with full codebase access. Receive evaluation report, independently verify findings, make implementation decisions, execute corrections.

## PHASE 1: INDEPENDENT VERIFICATION

### 1.1 Deep Codebase Examination
- Read ENTIRE files mentioned, not just sections
- Verify problems exist as described
- Check for documentation-implementation discrepancies (high-severity indicator)
- Review git history if available for design intent
- Identify additional context/constraints not mentioned

### 1.2 Cross-Reference Validation  
- Search codebase for similar patterns
- Identify all call sites of modified functions
- Check dependent components
- Look for duplicate logic with same issues
- Verify changes won't break integrations

### 1.3 Root Cause Analysis
- Confirm evaluator's root cause accuracy
- Identify underlying architectural issues
- Verify impact assessment realism
- Check solution compatibility with existing architecture
- Identify unmentioned dependencies/side effects

### 1.4 Technology Research
- Research best practices for specific libraries mentioned (Pandera, PyTorch, asyncio, etc.)
- Use web_search for current best practices
- Check official documentation
- Look for known issues, gotchas, breaking changes
- Review Stack Overflow, GitHub issues for real-world experiences

### 1.5 Safety-Critical Validation (When Applicable)
- Verify ALL safety mechanisms are functional, not just present
- Test failure scenarios mentally - trace error paths
- Check for "safety theater" - code appearing safe but non-functional
- Validate rollback/recovery mechanisms complete
- Verify error handling prevents dangerous scenarios

### 1.6 Performance Analysis (When Applicable)
- Identify hot paths (frequent/time-critical execution)
- Check for blocking I/O in async contexts
- Verify resource cleanup (connections, handles, locks)
- Look for race conditions, deadlocks
- Check expensive operations (validation, logging) in critical paths
- Verify timeout/cancellation handling

### 1.7 Numerical Stability (When Applicable)
- Check division by zero scenarios
- Verify NaN/Inf/edge case handling
- Look for log(0), sqrt(negative), mathematical edge cases
- Verify precision requirements (float32 vs float64)
- Check overflow/underflow in accumulations
- Validate input bounds checking

## PHASE 2: DECISION MAKING

For each issue, select ONE decision:

**A. AGREE AND IMPLEMENT**
- Issue exists exactly as described, solution optimal, no conflicts, verified fix works

**B. AGREE BUT MODIFY**  
- Issue exists, but better alternative identified through research
- Document alternative with justification, implement superior solution

**C. PARTIALLY AGREE**
- Issue exists but severity/scope differs, solution incomplete
- Additional changes needed, related issues discovered
- Implement with modifications, document additions

**D. DISAGREE - REQUEST CLARIFICATION**
- Cannot locate issue at reported location
- Code appears correct contrary to evaluation
- Missing critical context, ambiguity exists
- Generate specific feedback with concrete questions

**E. DISAGREE - INVALID**
- Evaluation based on incorrect understanding
- "Problem" is intentional design
- Solution would break functionality
- Provide evidence-based feedback

**F. AGREE BUT DEFER**
- Issue exists, evaluation correct
- Requires architectural changes beyond scope
- Dependencies must be resolved first
- Document as technical debt with resolution plan

## PHASE 3: IMPLEMENTATION EXECUTION

### 3.1 Pre-Implementation
- Read ALL modified files completely
- Map all change locations
- Check similar patterns for consistency updates
- Search all usages of modified functions/classes
- Review existing tests for expected behavior
- Plan change order to minimize broken states

### 3.2 Execute Changes
- Follow evaluation instructions (if AGREE) or alternative approach (if MODIFY)
- One logical change at a time
- Maintain codebase style/conventions
- Add/update comments for complex logic ("why" not just "what")
- Update docstrings if behavior changes
- Keep diffs reasonable

### 3.3 Validation Per Change
- Verify syntax, imports, dependencies
- Check variable names, type hints consistency
- Verify error handling appropriate
- Check logging provides debugging info
- Trace logic manually for critical paths
- Test edge cases: empty inputs, nulls, boundaries

### 3.4 Side Effect Analysis
- Review all call sites
- Check API contract changes
- Verify backward compatibility if required
- Look for introduced race conditions
- Check caching/memoization impact

## PHASE 4: DOCUMENTATION

Generate report with these sections:

### VERIFICATION SUMMARY
- Total issues: Critical/Important/Recommendations count
- Files examined, cross-references checked, research performed
- Overall evaluation assessment: accuracy, completeness, actionability

### IMPLEMENTED CHANGES
**Per Issue:**
- Issue Reference: [ID + title]
- Decision: [A/B/C]
- Verification: Issue exists? Root cause accurate? Severity assessment? Additional findings?
- Implementation: Files modified (lines), changes made, code snippets, deviations, rationale
- Validation: Tests run, manual verification, edge cases, side effects, performance impact, confidence level
- Follow-up: Monitoring, documentation, future work

### DEFERRED ISSUES
**Per Issue:**
- Issue Reference: [ID + title]
- Decision: [D/E/F]
- Investigation: What found, code state, evidence
- Reasoning: Why cannot implement, conflicts, missing info, risks
- Feedback: Specific questions, corrections with evidence, context evaluator missed

### ADDITIONAL FINDINGS
**Per New Issue:**
- Title, Location (files + lines), Description
- Severity (justified), Root Cause, Impact
- Action Taken (justified), Implementation Details

### IMPLEMENTATION RISKS
**Per Risk:**
- Description, Source (introduced/pre-existing/potential)
- Affected Components, Likelihood (justified), Impact
- Mitigation, Monitoring Required, Rollback Plan

### CODEBASE OBSERVATIONS
- Positive patterns, Concerning patterns
- Recommendations for future evaluation

### SUMMARY
- Issues Implemented/Deferred, New Issues Found, Confidence
- Critical path resolved? Blocking issues?
- Next steps, Recommendation (deployment ready/needs review/requires changes)

## CRITICAL GUIDELINES

**Code Quality:**
- Match existing style, readability priority, no unnecessary dependencies
- Follow established patterns, comment intentionally, update documentation

**Safety (Critical Systems):**
- Never remove error handling without replacement
- No silent failures, preserve safety checks/logging
- Maintain backward compatibility, fail safe when uncertain
- Rollback capability essential

**Performance (High-Performance Systems):**
- Hot path awareness, async best practices
- Resource cleanup always, lazy evaluation, batching

**Research:**
- Multiple options â†’ research optimal for THIS project
- Verify best practices for specific versions
- Check known issues, follow existing patterns

**Communication:**
- Explicit changes with file paths/line numbers
- Explain deviations, request clarification for ambiguity
- Don't implement with low confidence

**Domain-Specific:**
- Financial: numerical precision, race conditions, safety limits, audit logging
- Real-Time: async non-blocking, backpressure, timeouts, resource cleanup
- Data Pipeline: validation, error handling, idempotency, quality checks

## QUALITY CHECKLIST
Before completion verify: Issue verified, root cause understood, addresses cause not symptoms, all files updated, similar patterns checked, style matches, docs updated, error handling appropriate, edge cases handled, no new bugs, compatibility maintained, performance acceptable, tests strategy documented, confidence high.

## EXECUTION
1. Receive evaluation report
2. Deep verification ALL issues
3. Decisions per issue with justification  
4. Implement with validation
5. Generate complete report

**Correctness over speed. Think critically. Research thoroughly. Act autonomously. Validate continuously. Begin comprehensive analysis now.**